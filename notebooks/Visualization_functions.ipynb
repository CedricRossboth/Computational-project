{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e168c385-0089-4ce2-81bc-dba9215b8e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from re import sub\n",
    "from sympy import sympify, Symbol, exp, log, I, pi, zoo, latex, N, simplify\n",
    "from scipy.optimize import curve_fit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f2786a5-c7d9-4765-b821-40ee466895bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_average(values, index):\n",
    "    # This function computes the average of numeric values at a specified index in a list of lists, while safely filtering out entries that are not lists, too short, or have non-numeric values at that index.\n",
    "\n",
    "    selected = [\n",
    "        item[index] for item in values\n",
    "        if isinstance(item, list)\n",
    "        and len(item) > index\n",
    "        and isinstance(item[index], (int, float))\n",
    "    ]\n",
    "    return sum(selected) / len(selected) if selected else float('nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c5f615a-b901-4d3f-972c-69b3ef8ff415",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_model_string(model_str):\n",
    "    # It turns what was stored in the text file as a string back to a list with a sympy object for the expression obtained during the training.\n",
    "\n",
    "    # Locals for SymPy expressions\n",
    "    converter = {\n",
    "        'exp': exp,\n",
    "        'log': log,\n",
    "        'I': I,\n",
    "        'pi': pi\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Replace np.float64(...) with just the number\n",
    "        model_str_clean = sub(r'np\\.float64\\((.*?)\\)', r'\\1', model_str)\n",
    "        \n",
    "        # Find the last comma (before the expression) and quote the expression part\n",
    "        if model_str_clean.strip().startswith('[') and model_str_clean.strip().endswith(']'):\n",
    "            inside = model_str_clean[1:-1]  # Remove surrounding []\n",
    "            parts = inside.rsplit(',', 1)  # Split on last comma\n",
    "            expression_quoted = f'\"{parts[1].strip()}\"'\n",
    "            model_str_safe = f'[{parts[0].strip()}, {expression_quoted}]'\n",
    "        else:\n",
    "            raise ValueError(\"Malformed model string\")\n",
    "\n",
    "        # Safely evaluate the list\n",
    "        parts = eval(model_str_safe, {\"__builtins__\": None}, {})\n",
    "\n",
    "        # Convert the last part (string) to a SymPy expression\n",
    "        parts[-1] = sympify(parts[-1], locals=converter)\n",
    "\n",
    "        return parts\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing model:\\n{model_str}\\n{e}\\n\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8972ab50-c037-4a99-ae45-4051d04e4370",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(data):\n",
    "    # Solves an occuring issue in visualize_set.\n",
    "    return data  # no copy, just returns the original reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e2ff9c6-5106-4b57-a426-85b27c4bf704",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_set(data_set, ax, estimator = False, add_legend=True):\n",
    "    # This allows to see how the data is spread. The raw data can be seen for any of the X_train, X_test or the total data X set. If a gplearn or a lamdified object is given the predicted data is visualized.\n",
    "\n",
    "    # The data must be reloaded each time as a clean look was wanted for the symbolic_regressor.ipynb file. It slows the process but makes the other file clearer. \n",
    "    Energies_train = pd.read_csv('Fe_Octhedral_complexes_rel_m_splitting.csv')\n",
    "    Energies_test = pd.read_csv('real_world_energies_and_relative_metal_radius.csv')\n",
    "    Energies_total = pd.concat([Energies_train,  Energies_test], ignore_index=True, axis=0)\n",
    "\n",
    "    X_train = Energies_train[['rel_m']]\n",
    "    X_test = Energies_test[['rel_m']]\n",
    "    X = Energies_total[['rel_m']]\n",
    "    \n",
    "\n",
    "    number_of_plots = 4 # (Multiplicities 1,2, 5 and 6)\n",
    "\n",
    "    # When inputting an estimator in the function the name of the plot changes from the Reference data (data points calculated using DFT) to Estimated data (data points estimated with the expression obtained)\n",
    "    if not estimator:\n",
    "        ax.set_prop_cycle('color', plt.cm.cool(np.linspace(0, 1, number_of_plots)))\n",
    "        title = 'Reference data'\n",
    "    else:\n",
    "        ax.set_prop_cycle('color', plt.cm.hot(np.linspace(0, 0.9, number_of_plots)))\n",
    "        title = 'Estimated data'\n",
    "\n",
    "    # Labels x and y axes.\n",
    "    ax.set(xlabel = 'Spin splitting energy [kcal/mol]', ylabel = r'$\\mathrm{r_{rel}}$')\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    # Identifies which set must be considered\n",
    "    if data_set.equals(X_train):\n",
    "        Energies = Energies_train\n",
    "    elif data_set.equals(X_test):\n",
    "        Energies = Energies_test\n",
    "    elif data_set.equals(X):\n",
    "        Energies = Energies_total\n",
    "\n",
    "\n",
    "    # Plots the data points for one multiplicity at a time\n",
    "    for Multiplicity in [1,2,5,6]:\n",
    "        splitting_E = []\n",
    "        r_rel = []\n",
    "\n",
    "        # Loops over the complexes in the data set considered.\n",
    "        for idx in range(len(Energies)):     \n",
    "            \n",
    "            # If the multiplicity currently looked at corresponds to the multiplicity of the complex, the data point is added to the list of data points to scatter in this current loop.\n",
    "            if Multiplicity == Energies.loc[idx,'multiplicity']:\n",
    "                r_rel.append(Energies.at[idx, 'rel_m'])\n",
    "                \n",
    "                # If Reference data: takes the splitting energy in the dataset.\n",
    "                if not estimator:\n",
    "                    splitting_E.append(Energies.at[idx, 'splitting'])\n",
    "\n",
    "                # Else calculates it using the expression given. Two cases: the estimator is a numpy object (first case) or it is a gplearn object (second case).\n",
    "                else:\n",
    "                    if type(estimator).__name__ == \"function\":\n",
    "                        splitting_E.append(estimator(Energies.at[idx, 'rel_m']))\n",
    "                    else:\n",
    "                        splitting_E.append(estimator.predict([[Energies.at[idx, 'rel_m']]]))\n",
    "\n",
    "        if not estimator:\n",
    "            label = f'{Multiplicity} (data)'\n",
    "        else:\n",
    "            label = f'{Multiplicity} (estimated)'\n",
    "\n",
    "        # After the loop over all the complexes the points corresponding to the multiplicity are scattered on the plot with the color given by the prop_cycle.\n",
    "        ax.scatter(splitting_E, r_rel, edgecolors='k', linewidth=0.5, label=label)\n",
    "            \n",
    "                \n",
    "\n",
    "            \n",
    "    if data_set.equals(X_train):\n",
    "        title += ': training set'\n",
    "    elif data_set.equals(X_test):\n",
    "        title += ': testing set'\n",
    "    elif data_set.equals(X):\n",
    "        title += ': total set'\n",
    "        \n",
    "    if add_legend == True:\n",
    "        ax.set_title(title)\n",
    "        plt.legend()\n",
    "    plt.grid(False)\n",
    "    # Add a subtle background color\n",
    "    ax.set_facecolor('#f9f9f9')\n",
    "    \n",
    "    # Remove spines for cleaner appearance\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    \n",
    "    plt.savefig('Images/Training_set.svg', bbox_inches='tight')\n",
    "\n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2502cdb-3d4b-483e-9bcc-7135b199c0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_estimator(estimator):\n",
    "    # Allows to visualize how the estimator performs against the test set. Its expression is plotted at the top and a parity plot is also made. \n",
    "\n",
    "    # The data must be reloaded each time as a clean look was wanted for the symbolic_regressor.ipynb file. However, it makes the process slower. \n",
    "    Energies_test = pd.read_csv('real_world_energies_and_relative_metal_radius.csv')\n",
    "    X_test = Energies_test[['rel_m']]\n",
    "    y_test = Energies_test['splitting']\n",
    "    \n",
    "    # Locals for SymPy expressions\n",
    "    converter = {\n",
    "        'sub': lambda x, y : x - y,\n",
    "        'div': lambda x, y : x/y,\n",
    "        'mul': lambda x, y : x*y,\n",
    "        'add': lambda x, y : x + y,\n",
    "        'neg': lambda x    : -x,\n",
    "        'pow3': lambda x : x**3,\n",
    "        'root3': lambda x : x**(1/3)\n",
    "    }\n",
    "\n",
    "    \n",
    "    # The plots are created and the labels are added.\n",
    "    fig, axs = plt.subplots(ncols=2, figsize=(10, 6), dpi = 150)  # Two horizontal subplot\n",
    "    (ax1, ax2) = axs\n",
    "    \n",
    "    ax1.set_xlabel('Spin splitting energy [kcal/mol]', fontsize= 12)\n",
    "    ax1.set_ylabel(r'$\\mathrm{r_{rel}}$', fontsize= 12)\n",
    "    ax1.set_title('Visualization of predictor', fontsize= 14)\n",
    "\n",
    "    ax2.set_xlabel('DFT calculated spin splitting energy [kcal/mol]', fontsize= 12)\n",
    "    ax2.set_ylabel('Predicted spin splitting energy [kcal/mol]', fontsize=12)\n",
    "    ax2.set_title('Accuracy of fit', fontsize= 14)\n",
    "    plt.subplots_adjust(right=0.95)\n",
    "\n",
    "    \n",
    "    # The predicted data and the testing data are put on the first axis without their legen being added.\n",
    "    visualize_set(X_test, ax1, estimator = False, add_legend = False)\n",
    "    visualize_set(X_test, ax1, estimator = estimator, add_legend = False)\n",
    "\n",
    "    # The analytical expression is plotted on the first axis.\n",
    "    y_see_function = pd.DataFrame(np.arange(min(X_test['rel_m']), max(X_test['rel_m']), 0.001))\n",
    "    \n",
    "    if type(estimator).__name__ == \"function\":\n",
    "        x_see_function = [estimator(y_see_function[0][i]) for i in range(len(y_see_function))]\n",
    "    else:\n",
    "        x_see_function = estimator.predict(y_see_function)\n",
    "    \n",
    "    ax1.plot(x_see_function, y_see_function, linestyle='-', color = 'k', markersize=3, zorder=2)\n",
    "\n",
    "    # The parity plot is created here by recreating the predicted values from the estimator. It can both be a gplearn object or a numpy expression.\n",
    "    if type(estimator).__name__ == \"function\":\n",
    "        y_pred = [estimator(X_test['rel_m'][i]) for i in range(len(X_test['rel_m']))]\n",
    "    else:\n",
    "        y_pred = estimator.predict(X_test)\n",
    "\n",
    "    ax2.scatter(list(y_test), y_pred, color = 'b', edgecolors='k', linewidth=0.5)\n",
    "\n",
    "    # The information on how well the model performed are calculated here.\n",
    "    popt_lin_reg, pcov_lin_reg = curve_fit(linear_regression, list(y_test), list(y_pred), bounds = (0, np.inf))\n",
    "    y_fit_lin_reg = linear_regression(y_test, *popt_lin_reg)\n",
    "    r_squared_lin_reg = r_squared_function(y_fit_lin_reg, y_pred)\n",
    "    MAE = estimator._program.raw_fitness_\n",
    "\n",
    "    # The linear regression curve is drawn on the second plot for only two points for efficiency purposes.\n",
    "    plot_limits =  [1.15*min(min(y_test), min(y_pred)), 1.15*max(max(y_test), max(y_pred))]\n",
    "    y_fit_lin_reg_plot = linear_regression(plot_limits, *popt_lin_reg)\n",
    "    ax2.plot(plot_limits, y_fit_lin_reg_plot, color = 'k', linewidth = 0.5, label = r'linear regression: $\\mathrm{R^2 = %.3f}$' '\\n'\n",
    "                                                                            r'linear regression: $\\mathrm{slope = %.3f}$' '\\n' \n",
    "                                                                            r' MAE = %.3f'% (r_squared_lin_reg, popt_lin_reg[0], MAE))\n",
    "\n",
    "    # Legends are added and the secon plot has its axes set to a certain size to show best the slope of the estimated data.\n",
    "    ax2.set_xlim(plot_limits)\n",
    "    ax2.set_ylim(plot_limits)\n",
    "    ax1.legend(title = 'multiplicity', ncol=2, fontsize = 10)\n",
    "    ax2.legend(fontsize = 10, loc = 'upper left')\n",
    "\n",
    "    \n",
    "    # The expression is added at the top of the plot.\n",
    "    expr_str = sympify(estimator._program.__str__(), locals = converter)\n",
    "    # Defines the replacement symbol\n",
    "    X = Symbol('X0') \n",
    "    r_rel = Symbol('r_{rel}')\n",
    "    rounded_expr = expr_str.xreplace({\n",
    "        **{n: round(N(n), 3) for n in expr_str.atoms() if n.is_Float},\n",
    "        X: r_rel\n",
    "    })\n",
    "    latex_expr = r\"\\Delta E = \" + latex(rounded_expr)\n",
    "    fig.suptitle(f\"${latex_expr}$\", y = 1.0, fontsize= 20)\n",
    "\n",
    "    \n",
    "    # Makes the plot look neat by setting a beackground color and removing the top and right black lines of the plot.\n",
    "    ax2.set_facecolor('#f9f9f9')\n",
    "    ax2.spines['top'].set_visible(False)\n",
    "    ax2.spines['right'].set_visible(False)\n",
    "\n",
    "    # Saves the image according to the size of the plot accounting for the suptitle.\n",
    "    plt.tight_layout(rect=[0, 0, 1, 1]) \n",
    "    plt.savefig(\"Images/First_good_model.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "    return MAE, r_squared_lin_reg, popt_lin_reg[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72746866-c016-4697-aacf-bb71f9713171",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MAE_r_squared_slope_of_est(estimator):\n",
    "    # The name of the function is self explanatory, from an estimator you obtain the MAE, the r squared and the slope of the parity plot compared to the testing data.\n",
    "    # This function only works for gplearn estimator objects. This function can be used in the obtention of the different models as to have a faster calculation process.\n",
    "    \n",
    "    # The data must be reloaded each time as a clean look was wanted for the symbolic_regressor.ipynb file. It slows the process but makes the other file clearer. \n",
    "    Energies_test = pd.read_csv('real_world_energies_and_relative_metal_radius.csv')\n",
    "    X_test = Energies_test[['rel_m']]\n",
    "    y_test = Energies_test['splitting']\n",
    "\n",
    "    # The predicted data is obtained from the expression of the model and the testing set. \n",
    "    y_pred = estimator.predict(X_test)\n",
    "\n",
    "    # The characteristics are calculated\n",
    "    popt_lin_reg, pcov_lin_reg = curve_fit(linear_regression, list(y_test), list(y_pred), bounds = (0, np.inf))\n",
    "    y_fit_lin_reg = linear_regression(y_test, *popt_lin_reg)\n",
    "    r_squared_lin_reg = r_squared_function(y_fit_lin_reg, y_pred)\n",
    "    MAE = estimator._program.raw_fitness_\n",
    "\n",
    "    return MAE, r_squared_lin_reg, popt_lin_reg[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e065bc8d-bca8-4247-858a-a9b39f47bd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_models(df, type_tuning):\n",
    "    \"\"\"\n",
    "    This function creates pie charts of how well the trained model performed. The models are \"ranked\" based on if their expression contained a singularity in the range of training/testing data, \n",
    "    if the r squared is positive, if the expression contained unphysical terms (infinite or imaginary values) an if the ground spin state prediction was well done.\n",
    "    \n",
    "    The dataframe given has this structure: its columns are the name of the models (I, II; III, IV, V and VI) and the rows are lists which are in this order: \n",
    "    singularity_match, r_squared, percentage_same_sign(y_test_sorted, y_pred), MAE, slope, expr_str\n",
    "    \"\"\"\n",
    "    \n",
    "    for key in list(df.keys()):\n",
    "        df[key] = df[key].apply(parse_model_string)\n",
    "\n",
    "    data_list = []\n",
    "    good_models = []\n",
    "\n",
    "    # Goes through all the models of a certain type of training and counts how many models fill some criteria.\n",
    "    for model_type in df.keys():\n",
    "        data = [0, 0, 0, 0, 0, 0]\n",
    "        \n",
    "        for idx_model, model in enumerate(df[model_type]):\n",
    "            if model[0]:\n",
    "                data[0]+=1\n",
    "                continue\n",
    "                \n",
    "            if model[1] < 0:\n",
    "                data[1]+=1\n",
    "                continue\n",
    "\n",
    "            if model[5].has(I) or model[5].has(zoo):\n",
    "                data[2]+=1\n",
    "                continue\n",
    "                \n",
    "            if model[2] < 90:\n",
    "                data[3]+=1\n",
    "                continue\n",
    "\n",
    "            if model[2] < 98:\n",
    "                data[4]+=1\n",
    "                continue\n",
    "\n",
    "            # The models that don't correspond to any of those criteria are added to the category of good models.\n",
    "            data[5]+=1\n",
    "            good_models.append([model_type, idx_model, model])\n",
    "\n",
    "        data_list.append(np.array(data)/np.sum(data)*100)\n",
    "\n",
    "    # Defines the title of the pie charts and the labels to be put on the right side of the plot. \n",
    "    titles = ['I', 'II', 'III', 'IV', 'V', 'VI']\n",
    "    global_labels = ['Singularity', r'negative $\\mathrm{R^2}$', 'Imaginary part/infinite term', 'Spin agreement < 90%', '90% ≤ Spin agreement < 98%', 'Good models']\n",
    "    colors = plt.cm.cool(np.linspace(0, 1, len(global_labels)))\n",
    "    \n",
    "    # Create 2x3 subplots and adds the pie charts to each.\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(8, 6))\n",
    "    axes = axes.flatten()  # Flatten to make indexing easier\n",
    "    \n",
    "    for i in range(6):\n",
    "        wedges, _ = axes[i].pie(data_list[i], colors=colors)\n",
    "        axes[i].set_title(f\"{titles[i]}: MAE = {round(safe_average(df[titles[i]], 3), 2)}\")\n",
    "    \n",
    "    # Global legend (top right of figure)\n",
    "    fig.legend(wedges, global_labels, loc='upper right', bbox_to_anchor=(1.3, 1), title='Categories')\n",
    "\n",
    "    # Changes the title according to the two type of models trained. It has to be entered by hand which experience it was.\n",
    "    if type_tuning == 'Add' or type_tuning == 'add':\n",
    "        suptitle = \"Distribution of models based on the tuned\\n parameters on Add MAE\"\n",
    "    if type_tuning == 'Exp' or type_tuning == 'exp':\n",
    "        suptitle = \"Distribution of models based on the tuned\\n parameters on Exp spike/cst\"\n",
    "\n",
    "    fig.suptitle(suptitle, fontsize=18)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    fig.savefig(\"Images/Pie_chart_Exp.svg\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "    return good_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec831521-aa25-43cd-b462-134e5d7238a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(L, A):\n",
    "    # It is not a linear regression per se but it is used with the curve_fit function to determine \n",
    "    # the best linear relation for the parity plot of the estimated data and the tested data.\n",
    "    return A*np.array(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40c89ac4-2bb4-4e6c-9cfb-10eefe66cc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_squared_function(y_data, y_fit):\n",
    "    # Gets the r squared from the fitted data (y_fit) and the reference data (y_data).\n",
    "    residuals = y_data - y_fit\n",
    "    ss_res = np.sum(residuals**2)\n",
    "    \n",
    "    #You can get the total sum of squares (ss_tot) with\n",
    "    ss_tot = np.sum((y_data-np.mean(y_data))**2)\n",
    "\n",
    "    #And finally, the r_squared-value with,\n",
    "    r_squared = 1 - (ss_res / ss_tot)\n",
    "\n",
    "    return r_squared"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
